{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chatbot w/ Bard API"],"metadata":{"id":"ib1AG4FukFEA"}},{"cell_type":"markdown","source":["## Setup Environment"],"metadata":{"id":"OaGoxClLXoP6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kK_XBJ1XHGh","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1688628828771,"user_tz":-330,"elapsed":15350,"user":{"displayName":"Gopal Sharma","userId":"12839894256006393174"}},"outputId":"8e1481ec-577d-4e45-d3ed-242672f9dfaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bardapi\n","  Downloading bardapi-0.1.24-py3-none-any.whl (19 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bardapi) (2.27.1)\n","Collecting deep-translator (from bardapi)\n","  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting colorama (from bardapi)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting httpx[http2] (from bardapi)\n","  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-cloud-translate==2.0.1 (from bardapi)\n","  Downloading google_cloud_translate-2.0.1-py2.py3-none-any.whl (90 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-api-core[grpc]<2.0.0dev,>=1.15.0 (from google-cloud-translate==2.0.1->bardapi)\n","  Downloading google_api_core-1.34.0-py3-none-any.whl (120 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.1.0 (from google-cloud-translate==2.0.1->bardapi)\n","  Downloading google_cloud_core-1.7.3-py2.py3-none-any.whl (28 kB)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator->bardapi) (4.11.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bardapi) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bardapi) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->bardapi) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bardapi) (3.4)\n","Collecting httpcore<0.18.0,>=0.15.0 (from httpx[http2]->bardapi)\n","  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]->bardapi) (1.3.0)\n","Collecting h2<5,>=3 (from httpx[http2]->bardapi)\n","  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator->bardapi) (2.4.1)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->bardapi) (1.59.1)\n","Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->bardapi) (3.20.3)\n","Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->bardapi) (2.17.3)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->bardapi) (1.56.0)\n","Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->bardapi) (1.48.2)\n","Collecting google-auth<3.0dev,>=1.25.0 (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->bardapi)\n","  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-core<2.0dev,>=1.1.0->google-cloud-translate==2.0.1->bardapi) (1.16.0)\n","Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]->bardapi)\n","  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n","Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]->bardapi)\n","  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore<0.18.0,>=0.15.0->httpx[http2]->bardapi)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx[http2]->bardapi) (3.7.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx[http2]->bardapi) (1.1.1)\n","Collecting cachetools<5.0,>=2.0.0 (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->bardapi)\n","  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->bardapi) (0.3.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->bardapi) (67.7.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->bardapi) (4.9)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->bardapi) (0.5.0)\n","Installing collected packages: hyperframe, hpack, h11, colorama, cachetools, httpcore, h2, google-auth, deep-translator, httpx, google-api-core, google-cloud-core, google-cloud-translate, bardapi\n","  Attempting uninstall: cachetools\n","    Found existing installation: cachetools 5.3.1\n","    Uninstalling cachetools-5.3.1:\n","      Successfully uninstalled cachetools-5.3.1\n","  Attempting uninstall: google-auth\n","    Found existing installation: google-auth 2.17.3\n","    Uninstalling google-auth-2.17.3:\n","      Successfully uninstalled google-auth-2.17.3\n","  Attempting uninstall: google-api-core\n","    Found existing installation: google-api-core 2.11.1\n","    Uninstalling google-api-core-2.11.1:\n","      Successfully uninstalled google-api-core-2.11.1\n","  Attempting uninstall: google-cloud-core\n","    Found existing installation: google-cloud-core 2.3.2\n","    Uninstalling google-cloud-core-2.3.2:\n","      Successfully uninstalled google-cloud-core-2.3.2\n","  Attempting uninstall: google-cloud-translate\n","    Found existing installation: google-cloud-translate 3.11.1\n","    Uninstalling google-cloud-translate-3.11.1:\n","      Successfully uninstalled google-cloud-translate-3.11.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-auth-oauthlib 1.0.0 requires google-auth>=2.15.0, but you have google-auth 1.35.0 which is incompatible.\n","google-cloud-storage 2.8.0 requires google-cloud-core<3.0dev,>=2.3.0, but you have google-cloud-core 1.7.3 which is incompatible.\n","google-colab 1.0.0 requires google-auth==2.17.3, but you have google-auth 1.35.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed bardapi-0.1.24 cachetools-4.2.4 colorama-0.4.6 deep-translator-1.11.4 google-api-core-1.34.0 google-auth-1.35.0 google-cloud-core-1.7.3 google-cloud-translate-2.0.1 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-0.17.3 httpx-0.24.1 hyperframe-6.0.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]}}},"metadata":{}}],"source":["!pip install bardapi"]},{"cell_type":"code","source":["from bardapi import Bard\n","import os\n","import time"],"metadata":{"id":"VF1KrU4urUJP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace XXXX with the values you get from __Secure-1PSID\n","os.environ['_BARD_API_KEY']=\"Insert-Key-Here\""],"metadata":{"id":"-rY42uSnXbOG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Simplest Chatbot"],"metadata":{"id":"SQW9eevUj8ly"}},{"cell_type":"code","source":["# Set your input text\n","# input_text = \"Why is the sky blue?\"\n","input_text = \"What is the current status of Ashes Series 2023\"\n","print(Bard().get_answer(input_text)['content'])"],"metadata":{"id":"DFnfJtYbXd_E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688629268413,"user_tz":-330,"elapsed":6265,"user":{"displayName":"Gopal Sharma","userId":"12839894256006393174"}},"outputId":"8bb6b039-58d8-462a-aae8-3efa3546bd8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["As of July 6, 2023, Australia is leading the 2023 Ashes series 2-0. The first Test was played at Edgbaston, Birmingham, and Australia won by 2 wickets. The second Test was played at Lord's, London, and Australia won by 43 runs. The third Test is currently being played at Headingley, Leeds, and England needs to win to keep the series alive.\n","\n","The full schedule for the 2023 Ashes series is as follows:\n","\n","* **1st Test:** Edgbaston, Birmingham, June 16-20, 2023\n","* **2nd Test:** Lord's, London, June 28-July 2, 2023\n","* **3rd Test:** Headingley, Leeds, July 6-10, 2023\n","* **4th Test:** Emirates Old Trafford, Manchester, July 19-23, 2023\n","* **5th Test:** The Oval, London, July 27-31, 2023\n","\n","If Australia wins the third Test, they will retain the Ashes. If England wins the third Test, the series will be tied 2-2, and the fifth Test will be a decider.\n"]}]},{"cell_type":"markdown","source":["## Custom Chatbot"],"metadata":{"id":"8NU1b3f5kdkc"}},{"cell_type":"markdown","source":["### Text Summarizer"],"metadata":{"id":"x0pzfK-gkzJU"}},{"cell_type":"code","source":["!pip install pypdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6krlXbIVwsZe","executionInfo":{"status":"ok","timestamp":1688629502382,"user_tz":-330,"elapsed":6254,"user":{"displayName":"Gopal Sharma","userId":"12839894256006393174"}},"outputId":"858b28ae-c7fa-46ab-e8b3-b8db5ec3b533"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pypdf\n","  Downloading pypdf-3.12.0-py3-none-any.whl (254 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/254.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m245.8/254.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pypdf\n","Successfully installed pypdf-3.12.0\n"]}]},{"cell_type":"code","source":["from pypdf import PdfReader"],"metadata":{"id":"P6-zH35Dw0Xm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ptCfCdRprdOH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688629616810,"user_tz":-330,"elapsed":19410,"user":{"displayName":"Gopal Sharma","userId":"12839894256006393174"}},"outputId":"44e687f7-4884-4549-955e-01f79a475f42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["directory = '/content/drive/MyDrive/1-Build-a-Chatbot/'\n","filename  = 'Attention-Is-All-You-Need.pdf'\n","# filename  = 'Art-of-War.pdf'"],"metadata":{"id":"n6d42reCXq7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating a pdf file object\n","pdfFileObject = open(directory+filename, 'rb')\n","# creating a pdf reader object\n","pdfReader = PdfReader(pdfFileObject)\n","text=[]\n","summary=' '\n","#Storing the pages in a list\n","for i in range(0,len(pdfReader.pages)):\n","  # creating a page object\n","  pageObj = pdfReader.pages[i].extract_text()\n","  pageObj= pageObj.replace('\\t\\r','')\n","  pageObj= pageObj.replace('\\xa0','')\n","  # extracting text from page\n","  text.append(pageObj)"],"metadata":{"id":"gYpNlLqcZVyw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge multiple page - to reduce API Calls\n","def join_elements(lst, chars_per_element):\n","    new_lst = []\n","    for i in range(0, len(lst), chars_per_element):\n","        new_lst.append(''.join(lst[i:i+chars_per_element]))\n","    return new_lst\n","\n","# Option to keep x elements per list element\n","new_text = join_elements(text, 3)\n","\n","print(f\"Original Pages = \",len(text))\n","print(f\"Compressed Pages = \",len(new_text))"],"metadata":{"id":"0y8iM7trjKdM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688629980544,"user_tz":-330,"elapsed":452,"user":{"displayName":"Gopal Sharma","userId":"12839894256006393174"}},"outputId":"59cab736-bb6a-4125-d4a5-46594bcd97f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Pages =  15\n","Compressed Pages =  5\n"]}]},{"cell_type":"code","source":["def get_completion(prompt):\n","  response = Bard().get_answer(prompt)['content']\n","  return response"],"metadata":{"id":"AJ_lIoBzX267"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(len(new_text)):\n","  prompt =f\"\"\"\n","  Your task is to act as a Text Summariser.\n","  I'll give you text from  pages of a book from beginning to end.\n","  And your job is to summarise text from these pages in less than 100 words.\n","  Don't be conversational. I need a plain 100 word answer.\n","  Text is shared below, delimited with triple backticks:\n","  ```{text[i]}```\n","  \"\"\"\n","  try:\n","    response = get_completion(prompt)\n","  except:\n","    response = get_completion(prompt)\n","  print(response)\n","  summary= summary+' ' +response +'\\n\\n'\n","  # result.append(response)\n","  time.sleep(19)  #You can query the model only 3 times in a minute for free, so we need to put some delay"],"metadata":{"id":"CxpOAcNuZV3y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688630202480,"user_tz":-330,"elapsed":132479,"user":{"displayName":"Gopal Sharma","userId":"12839894256006393174"}},"outputId":"9bd843bc-b493-44fe-cc37-962bb17c123e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sure, here is the summary of the text you provided:\n","\n","The paper \"Attention Is All You Need\" proposes a new Transformer architecture for sequence transduction tasks. The Transformer architecture is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show that the Transformer models are superior in quality, more parallelizable, and require significantly less time to train than previous models. The Transformer architecture also generalizes well to other tasks, such as English constituency parsing.\n","\n","Here are some of the key points of the paper:\n","\n","* The Transformer architecture is based on self-attention, which allows the model to attend to different parts of the input sequence.\n","* The Transformer architecture is more parallelizable than previous models, which makes it faster to train.\n","* The Transformer architecture achieves state-of-the-art results on two machine translation tasks.\n","* The Transformer architecture generalizes well to other tasks, such as English constituency parsing.\n","\n","The paper \"Attention Is All You Need\" is a significant contribution to the field of natural language processing. The Transformer architecture is a powerful new tool for sequence transduction tasks, and it is likely to be used in a wide variety of applications.\n","\n","I hope this summary is helpful. Please let me know if you have any other questions.\n","Response Error: b')]}\\'\\n\\n39\\n[[\"wrb.fr\",null,null,null,null,[13]]]\\n58\\n[[\"di\",5974],[\"af.httprm\",5974,\"-961894757202100084\",0]]\\n25\\n[[\"e\",4,null,null,134]]\\n'. \n","Temporarily unavailable due to traffic or an error in cookie values. Please double-check the cookie values and verify your network environment.\n","Sure, here is the summary of the text you provided:\n","\n","The Transformer model is a neural network architecture for natural language processing (NLP). It is composed of an encoder and decoder stack, each of which is a stack of six identical layers. The encoder stack has two sub-layers: a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network. The decoder stack has three sub-layers: the same two sub-layers as the encoder stack, plus a third sub-layer that performs multi-head attention over the output of the encoder stack.\n","\n","The attention function in the Transformer model maps a query and a set of key-value pairs to an output. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n","\n","The Transformer model has been shown to be very effective for a variety of NLP tasks, including machine translation, text summarization, and question answering.\n","\n","Here is the summary in less than 100 words:\n","\n","The Transformer model is a neural network architecture for NLP that uses attention to learn long-range dependencies. It has been shown to be very effective for a variety of tasks, including machine translation, text summarization, and question answering.\n","\n","Is there anything else I can help you with?\n","Sure, here is the summary of the text you provided:\n","\n","**Scaled Dot-Product Attention**\n","\n","Scaled dot-product attention is a mechanism that computes the relevance of a query to a set of keys. It is a simple but effective attention mechanism that is used in many natural language processing tasks.\n","\n","The scaled dot-product attention algorithm works by first computing the dot product of the query and each key. The dot products are then divided by a scaling factor of **1 / sqrt(dk)**, where **dk** is the dimension of the keys. The softmax function is then applied to the scaled dot products to obtain the weights on the values.\n","\n","**Multi-Head Attention**\n","\n","Multi-head attention is a generalization of scaled dot-product attention that allows for multiple attention heads. Each attention head computes a separate attention weight for each value, which allows the model to attend to different aspects of the input.\n","\n","The multi-head attention algorithm works by first projecting the queries, keys, and values through separate linear layers. The projected queries, keys, and values are then passed through the scaled dot-product attention algorithm, resulting in a set of attention weights for each value. The attention weights are then concatenated and passed through another linear layer to obtain the final output.\n","\n","**Summary**\n","\n","Scaled dot-product attention and multi-head attention are two powerful attention mechanisms that are used in many natural language processing tasks. They are both relatively simple to implement, but they can be very effective at capturing the relationships between different parts of the input.\n","\n","I hope this summary is helpful. Let me know if you have any other questions.\n","Sure, here is the summary of the text you provided:\n","\n","The Transformer model uses multi-head attention to jointly attend to information from different representation subspaces at different positions. This allows the model to learn long-range dependencies and achieve state-of-the-art results on a variety of natural language processing tasks.\n","\n","The Transformer model also uses self-attention layers, which allow each position in the encoder or decoder to attend to all positions in the previous layer. This helps the model to learn the relationships between different parts of the input sequence.\n","\n","In addition to attention and self-attention layers, the Transformer model also uses position-wise feed-forward networks to improve its performance. These networks are applied to each position separately and identically, and they help the model to learn the relative or absolute position of the tokens in the sequence.\n","\n","Overall, the Transformer model is a powerful neural network architecture that has achieved state-of-the-art results on a variety of natural language processing tasks. It is a significant advance over previous models, and it is likely to play an important role in the future of natural language processing.\n","\n","Here are some of the key points from the text:\n","\n","* The Transformer model uses multi-head attention to learn long-range dependencies.\n","* The Transformer model also uses self-attention layers to learn the relationships between different parts of the input sequence.\n","* The Transformer model uses position-wise feed-forward networks to improve its performance.\n","* The Transformer model has achieved state-of-the-art results on a variety of natural language processing tasks.\n","\n","I hope this summary is helpful. Please let me know if you have any other questions.\n"]}]},{"cell_type":"code","source":["with open(directory +'/bard_summary.txt',\n","          'w') as out:\n","  out.write(summary)"],"metadata":{"id":"9SilyVEiX29Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Mock Interview"],"metadata":{"id":"X7egGmm2lylv"}},{"cell_type":"code","source":["# Build a Chatbot that takes a Mock Interview\n","# For a specific Job Role, provided as input by End-User."],"metadata":{"id":"kAV64iIplwPf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AI Teacher"],"metadata":{"id":"ww6orEprm-O0"}},{"cell_type":"code","source":["# This AI Teacher explains you topics in ELI-X level\n","# Where \"X\" is the age-appropriate learner level."],"metadata":{"id":"ktxJSTM-nABu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Code Reviewer"],"metadata":{"id":"9r81CAOhod28"}},{"cell_type":"code","source":["# AI helper that generates comprehensive review that cover code\n","# Clarity, structure, efficiency, adherence\n","# To best practices, and maintainability."],"metadata":{"id":"ItFGGhvQogCs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chatbot w/ ChatGPT API"],"metadata":{"id":"0Sz-Ga9lsdZY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xc7SQDTeQLr2"},"outputs":[],"source":["!pip install openai PyPDF2"]},{"cell_type":"code","source":["import openai\n","import PyPDF2\n","import os\n","import pandas as pd\n","import time"],"metadata":{"id":"H7TURFkhRzzy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filepath= \"/content/drive/MyDrive/1-Build-a-Chatbot/Attention-Is-All-You-Need.pdf\"\n","openai.api_key  = \"Insert-Key-Here\""],"metadata":{"id":"5j8ORkSRR8Hp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n","  messages = [{\"role\": \"user\", \"content\": prompt}]\n","  response = openai.ChatCompletion.create(\n","     model=model,\n","     messages=messages,\n","     temperature=0, # this is the degree of randomness of the model's output\n","  )\n","  return response.choices[0].message[\"content\"]"],"metadata":{"id":"Se1RoCM8R8OK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt =f\"\"\"Who is the Prime Minister of India?\"\"\"\n","response = get_completion(prompt)\n","print(response)\n","time.sleep(19)  #You can query the model only 3 times in a minute for free, so we need to put some delay"],"metadata":{"id":"8wUSQ_gqTbtM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating a pdf file object\n","pdfFileObject = open(filepath, 'rb')\n","# creating a pdf reader object\n","pdfReader = PyPDF2.PdfReader(pdfFileObject)\n","text=[]\n","summary=' '\n","#Storing the pages in a list\n","for i in range(0,len(pdfReader.pages)):\n","  # creating a page object\n","  pageObj = pdfReader.pages[i].extract_text()\n","  pageObj= pageObj.replace('\\t\\r','')\n","  pageObj= pageObj.replace('\\xa0','')\n","  # extracting text from page\n","  text.append(pageObj)"],"metadata":{"id":"PIW2BOdvRz23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(len(text)):\n","  prompt =f\"\"\"\n","  Your task is to extract relevant information from a text on the page of a book.\n","  This information will be used to create a book summary.\n","  Extract relevant information from the following text, which is delimited with triple backticks.\\\n","  Be sure to preserve the important details.\n","  Text: ```{text[i]}```\n","  \"\"\"\n","  try:\n","    response = get_completion(prompt)\n","  except:\n","    response = get_completion(prompt)\n","  print(response)\n","  summary= summary+' ' +response +'\\n\\n'\n","  result.append(response)\n","  time.sleep(19)  #You can query the model only 3 times in a minute for free, so we need to put some delay"],"metadata":{"id":"iDmLL0C2S35Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/1-Build-a-Chatbot/summary.txt',\n","          'w') as out:\n","  out.write(summary)"],"metadata":{"id":"x538eUQyTHhY"},"execution_count":null,"outputs":[]}]}